# LDA
rm(list = ls())
wd <- 'C:/Users/KLN/some_r'
setwd(wd)
source('util_fun.R')
library(slam)

input.dir <- 'C:/Users/KLN/some_r/data/nt_hist'
files.v <- dir(path = input.dir, pattern='.*txt')

# tokenize text in directory
maketext <- function(files,directory){
  text.word.l <- list()
  for(i in 1:length(files)){
    text.v <- scan(paste(directory, files[i], sep="/"), what="character", sep="\n") # read a file
    text.v <- paste(text.v, collapse=" ") # collapse lines
    text.lower.v <- tolower(text.v) # casefolding
    text.words.v <- strsplit(text.lower.v, "\\W") # tokenize
    text.words.v <- unlist(text.words.v) # transform list to vector
    text.words.v <- text.words.v[which(text.words.v!="")] # remove blanks
    text.word.l[[files[i]]] <- text.words.v # update list
  }
  names(text.word.l) <- gsub("\\..*","",files)
  return(text.word.l)
}  

text.word.l <- maketext(files.v,input.dir)
names(text.word.l) <- gsub("\\..*","",files.v)
text.l <- unlist(lapply(text.word.l,slice_text,10), recursive=FALSE)# why could slice size matter?
filenames.v <- gsub("\\..*","",names(text.l))

# create corpus from slices
library(tm)
text.cor <- Corpus(VectorSource(lapply(text.l, paste, collapse = " ")))
# clean and filter
text.cor <- tm_map(text.cor, removeNumbers)
text.cor <- tm_map(text.cor, removeWords, stopwords("english"))
text.cor <- tm_map(text.cor, stripWhitespace)
## create document term matrix
text.dtm <- DocumentTermMatrix(text.cor)
text.dtm <- docsparse(10,text.dtm)
summary(col_sums(text.dtm))

# prune dtm
prune <- function(dtm,mx,p){
  mx <- ceiling(dim(dtm)[1]*mx)
  dtm <- dtm[,slam::col_sums(as.matrix(dtm) > 0) < mx]
  dtm <- dtm[,col_sums(dtm) < quantile(slam::col_sums(dtm),p)]
  return(dtm)
}
text.dtm <- prune(text.dtm,.85,1)
summary(col_sums(text.dtm))

# extract Thomas
thom.idx <- filenames.v == 'Thomas'
thom.dtm <- text.dtm[thom.idx,]
text.dtm <- text.dtm[!thom.idx,]

# train topic model based latent dirichlet allocation
library(topicmodels) # Based on Blei's code
ls('package:topicmodels')# show functions in library
k = 10 # number of topics
seed <- 1234
mdl1 <- LDA(text.dtm, k = k, method = 'VEM', control = list(seed = seed))
## unpacking the model

# poportions parameter
alpha <- mdl1@alpha
print(alpha)
alpha# lexicon
lexicon <- mdl1@terms
head(lexicon)
# topics' word distribution
topicword.mat <- mdl1@beta
topicword.mat[1:10,1:10]
dim(topicword.mat)
terms(mdl1,10)# print the 10 most likely terms within each topic

# documents' topic distribution
doctopic.mat <- mdl1@gamma
dim(doctopic.mat)
doctopic.mat[1,]# topic saturation of document 1
row_sums(doctopic.mat)[1]
# plot document distribution
barplot(doctopic.mat[1,])
library(ggplot2)
dev.new()
i = 1 # document number to plot
  dt.df <- data.frame(x = 1:length(doctopic.mat[i,]), y = doctopic.mat[i,])
  ggplot(data = dt.df, aes(x = x, y = y)) +
  geom_bar(stat = "identity", colour ="#FF9999")+
  theme_minimal() +
  scale_x_discrete('Topic', breaks = 1:10, limits = as.character(1:10)) +
  ylab("Document weight") +
  labs(title = paste('Document',as.character(i), sep = ' '))
library(slam)

# calculate posteriors for words within each topic
mdl1post.l <- posterior(mdl1, text.dtm)
str(mdl1post.l)
# topic 1
tmp <- sort(mdl1post.l$terms[1,], decreasing = T)# word posteriors for each topic p(w|k)=Ï•kw
# and now the obligatory topic word cloud
library(wordcloud); require(RColorBrewer)
pal2 <- brewer.pal(8,"Dark2")
dev.new()
wordcloud(names(tmp[1:20]),tmp[1:20], scale=c(8,.2), random.order=FALSE, rot.per=.15, colors=pal2)

# perplexity


# infer topics of unseen documents
mdl1new.l <- posterior(mdl1, thom.dtm)
str(mdl1new.l)
barplot(mdl1new.l$topics[1,])# topic distribution for document 1 of unseen data

# model evaluation
perplexity(mdl1)
perplexity(mdl1,thom.dtm)

### scaling
# necessary
library(tm)
library(topicmodels)
# make life easier
library(plyr)
library(slam)

# build corpus
dd <- "C:/Users/KLN/some_r/data/kjv_books";
books.cor  <- Corpus(DirSource(dd, encoding = "UTF-8"), readerControl = list(language = "lat"))
names(books.cor) <- gsub("\\..*","",names(books.cor))# remove ending
filenames <- names(books.cor)
books.cor <- tm_map(books.cor, PlainTextDocument)
books.cor <- tm_map(books.cor, content_transformer(tolower))
books.cor <- tm_map(books.cor, removePunctuation)
books.cor <- tm_map(books.cor, removeNumbers)
books.cor <- tm_map(books.cor, removeWords, stopwords("english"))
books.cor <- tm_map(books.cor, stemDocument)
books.cor <- tm_map(books.cor, stripWhitespace)
# document term matrix
books.dtm <- DocumentTermMatrix(books.cor, control = list(minWordLength = 2))
dim(books.dtm)
books.dtm <- docsparse(2,books.dtm)
dim(books.dtm)
summary(col_sums(books.dtm))
books.dtm <- prune(books.dtm,.75,1)
summary(col_sums(books.dtm))

### estimate number of topics (optimal k estimation)
k = 100
#progress.bar <- create_progress_bar("text")
#progress.bar$init(k)
best.mdl <- list()
perplex.mat <- matrix(0,k-1,2)
for(i in 2:k){
  best.mdl[[i-1]] <- LDA(books.dtm, i)
  print(paste('k =',i, sep = ' '))
  #progress.bar$step()
}
save(best.mdl,file = 'estimate_k.RData')
#load('estimate_k.RData')
perplex.mat <- as.matrix(unlist(lapply(best.mdl, perplexity)))
plot(perplex.mat, main= 'Parameter estimation', xlab = 'k', ylab = 'Perplexity')

# unpack model
# ten most likely terms in each topic
terms(best.mdl[[19]],10)

# two dominant topics for all documents
top2 <- topics(best.mdl[[19]],2)
colnames(top2) <- filenames
print(top2)
